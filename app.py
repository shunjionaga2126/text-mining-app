import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from anthropic import Anthropic
from concurrent.futures import ThreadPoolExecutor, as_completed
import math

# â”€â”€â”€ å®šæ•°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BATCH_SIZE   = 20      # 1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§é€ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆæ•°
MAX_WORKERS  = 4       # ä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
MODEL_NAME   = "claude-3-7-sonnet-20250219"

# â”€â”€â”€ ãƒšãƒ¼ã‚¸è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="TagSense", layout="wide")
st.title("ğŸ—‚ï¸ TagSense â€” è‡ªå‹•ã‚¿ã‚°ä»˜ã‘ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# â”€â”€â”€ ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šAPIã‚­ãƒ¼å…¥åŠ› â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
api_key = st.sidebar.text_input(
    "ğŸ”‘ Claude APIã‚­ãƒ¼ã‚’å…¥åŠ›",
    type="password",
    help="Anthropicã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§å–å¾—ã—ãŸAPIã‚­ãƒ¼ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„"
)
if not api_key:
    st.sidebar.info("APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

# â”€â”€â”€ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded = st.file_uploader(
    "ğŸ“ ã‚³ãƒ¡ãƒ³ãƒˆCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…é ˆåˆ—: ã‚³ãƒ¡ãƒ³ãƒˆ, ä½œæˆæ—¥ï¼‰",
    type="csv"
)
if not uploaded:
    st.info("ã¾ãšã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    st.stop()

# â”€â”€â”€ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_csv(uploaded)
if "ã‚³ãƒ¡ãƒ³ãƒˆ" not in df.columns or "ä½œæˆæ—¥" not in df.columns:
    st.error("CSVã«å¿…é ˆåˆ—ã€Œã‚³ãƒ¡ãƒ³ãƒˆã€ã€Œä½œæˆæ—¥ã€ãŒã‚ã‚Šã¾ã›ã‚“")
    st.stop()
df["ä½œæˆæ—¥"] = pd.to_datetime(df["ä½œæˆæ—¥"], errors="coerce")

# â”€â”€â”€ ãƒãƒƒãƒè§£æé–¢æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_batch(comments: list[str], key: str) -> list[tuple[str, str]]:
    snippets = [str(c)[:200].replace("\n", " ") for c in comments]
    prompt = "ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã€ç•ªå·ä»˜ãã§æœ€å¤§3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã‚¿ã‚°ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
    for i, s in enumerate(snippets, 1):
        prompt += f"{i}. {s}\n"
    prompt += "\nå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:\n1. ã‚¿ã‚°1,ã‚¿ã‚°2,ã‚¿ã‚°3\nâ€¦\n"

    client = Anthropic(api_key=key)
    try:
        resp = client.messages.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.0,
        )
        raw = resp.content
        if isinstance(raw, list) and raw:
            raw = raw[0]
        text = raw.strip() if hasattr(raw, "strip") else str(raw)
    except Exception:
        return [("", "") for _ in comments]

    lines = [ln for ln in text.splitlines() if ln.strip()]
    results: list[tuple[str, str]] = []
    for line in lines[: len(comments)]:
        try:
            _, rest = line.split(".", 1)
            tags = [t.strip() for t in rest.split(",")][:3]
            results.append((", ".join(tags), ""))
        except:
            results.append((line.strip(), ""))
    while len(results) < len(comments):
        results.append(("", ""))
    return results

# â”€â”€â”€ ä¸¦åˆ—ãƒãƒƒãƒå®Ÿè¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_all(comments: list[str], key: str) -> list[tuple[str, str]]:
    total = len(comments)
    out: list[tuple[str, str]] = [("", "")] * total
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
        futures = {}
        for start in range(0, total, BATCH_SIZE):
            batch = comments[start : start + BATCH_SIZE]
            fut = exe.submit(analyze_batch, batch, key)
            futures[fut] = start
        for fut in as_completed(futures):
            start = futures[fut]
            try:
                batch_res = fut.result()
                for i, res in enumerate(batch_res):
                    out[start + i] = res
            except:
                for i in range(start, min(start + BATCH_SIZE, total)):
                    out[i] = ("", "")
    return out

# â”€â”€â”€ ã‚¿ã‚°ä»˜ã‘å®Ÿè¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ğŸ¤– ã‚¿ã‚°ä»˜ã‘ã‚’å®Ÿè¡Œ"):
    if not api_key:
        st.error("å…ˆã«APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        st.stop()
    comments = df["ã‚³ãƒ¡ãƒ³ãƒˆ"].astype(str).tolist()
    with st.spinner("è§£æä¸­â€¦ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„"):
        pairs = analyze_all(comments, api_key)
        df["ã‚¿ã‚°"] = [p[0] for p in pairs]
    st.success("âœ… ã‚¿ã‚°ä»˜ã‘ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

# â”€â”€â”€ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼†CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "ã‚¿ã‚°" in df.columns:
    # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹
    total      = len(df)
    tag_counts = (
        df["ã‚¿ã‚°"].str.split(",", expand=True)
          .stack().str.strip().value_counts()
    )
    unique_tags = len(tag_counts)
    top_count   = int(tag_counts.iloc[0]) if not tag_counts.empty else 0

    k1, k2, k3 = st.columns(3)
    k1.metric("ç·ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°", f"{total} ä»¶")
    k2.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ã‚°æ•°", f"{unique_tags}")
    k3.metric("ãƒˆãƒƒãƒ—ã‚¿ã‚°ä»¶æ•°", f"{top_count} ä»¶")

    st.markdown("---")

    # ã‚¿ã‚°ä»¶æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.subheader("ã‚¿ã‚°ä»¶æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    fig1, ax1 = plt.subplots(figsize=(6, 4))
    tag_counts.plot.bar(ax=ax1)
    ax1.set_ylabel("ä»¶æ•°")
    st.pyplot(fig1, use_container_width=True)

    # é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰
    st.subheader("ã‚¿ã‚°ã®é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰")
    weekly = (
        df.set_index("ä½œæˆæ—¥")["ã‚¿ã‚°"]
          .str.split(",", expand=True)
          .stack()
          .str.strip()
          .to_frame("tag")
    )
    # å…ˆé ­ãƒ¬ãƒ™ãƒ«ã®æ—¥æ™‚ã§ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    weekly = weekly.groupby(pd.Grouper(level=0, freq="W")).count()
    st.line_chart(weekly, use_container_width=True)

    # æ–°è¦ã‚¿ã‚°æ¤œå‡º
    first_date   = df["ä½œæˆæ—¥"].min()
    baseline_end = first_date + pd.Timedelta(days=6)
    all_tags     = set(tag_counts.index)
    baseline_tags= set(
        df[df["ä½œæˆæ—¥"] <= baseline_end]["ã‚¿ã‚°"]
          .str.split(",", expand=True)
          .stack().str.strip()
    )
    new_tags = sorted(all_tags - baseline_tags)
    if new_tags:
        st.subheader("éå»æœªå‡ºã‚¿ã‚°")
        for t in new_tags:
            st.write(f"- {t}")

    st.markdown("---")

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv_data = df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "ğŸ“¥ ã‚¿ã‚°ä»˜ãCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        csv_data,
        "tagged_results.csv",
        mime="text/csv"
    )
else:
    st.info("ã‚¿ã‚°ä»˜ã‘ã‚’å®Ÿè¡Œã™ã‚‹ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
