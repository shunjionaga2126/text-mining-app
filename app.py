import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from anthropic import Anthropic
from concurrent.futures import ThreadPoolExecutor, as_completed
import datetime

# â”€â”€â”€ å®šæ•°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BATCH_SIZE  = 20      # 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°
MAX_WORKERS = 4       # ä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
MODEL_NAME  = "claude-3-7-sonnet-20250219"

# â”€â”€â”€ ãƒšãƒ¼ã‚¸è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="TagSense", layout="wide")
st.title("ğŸ—‚ï¸ TagSense â€” è‡ªå‹•ã‚¿ã‚°ä»˜ã‘ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# â”€â”€â”€ ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šAPIã‚­ãƒ¼å…¥åŠ› â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
api_key = st.sidebar.text_input(
    "ğŸ”‘ Claude APIã‚­ãƒ¼ã‚’å…¥åŠ›",
    type="password",
    help="Anthropicã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§å–å¾—ã—ãŸAPIã‚­ãƒ¼ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„"
)
if not api_key:
    st.sidebar.warning("APIã‚­ãƒ¼ãŒãªã„ã¨ã‚¿ã‚°ä»˜ã‘ã§ãã¾ã›ã‚“")

# â”€â”€â”€ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded = st.file_uploader(
    "ğŸ“ ã‚³ãƒ¡ãƒ³ãƒˆCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…é ˆåˆ—: ã‚³ãƒ¡ãƒ³ãƒˆ, ä½œæˆæ—¥ï¼‰",
    type="csv"
)
if not uploaded:
    st.info("ã¾ãšã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    st.stop()

# â”€â”€â”€ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_csv(uploaded)
if "ã‚³ãƒ¡ãƒ³ãƒˆ" not in df.columns or "ä½œæˆæ—¥" not in df.columns:
    st.error("CSVã«å¿…é ˆåˆ—ã€Œã‚³ãƒ¡ãƒ³ãƒˆã€ã€Œä½œæˆæ—¥ã€ãŒã‚ã‚Šã¾ã›ã‚“")
    st.stop()
df["ä½œæˆæ—¥"] = pd.to_datetime(df["ä½œæˆæ—¥"], errors="coerce")

# â”€â”€â”€ ãƒãƒƒãƒè§£æé–¢æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_batch(comments: list[str], key: str) -> list[tuple[str, str]]:
    # å„ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…ˆé ­200æ–‡å­—ã«åˆ‡ã£ã¦æ”¹è¡Œé™¤å»
    snippets = [str(c)[:200].replace("\n", " ") for c in comments]
    prompt = "ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã€ç•ªå·ä»˜ãã§ã€Œãƒ¡ã‚¤ãƒ³ã‚¿ã‚°ã€ã¨ã€Œã‚µãƒ–ã‚¿ã‚°ï¼ˆæœ€å¤§2ã¤ï¼‰ã€ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
    for i, s in enumerate(snippets, 1):
        prompt += f"{i}. {s}\n"
    prompt += "\nå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:\n1. ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°: <ã‚¿ã‚°> | ã‚µãƒ–ã‚¿ã‚°: <ã‚¿ã‚°1>, <ã‚¿ã‚°2>\nâ€¦\n"

    client = Anthropic(api_key=key)
    try:
        resp = client.messages.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.0,
        )
        raw = resp.content
        if isinstance(raw, list) and raw:
            raw = raw[0]
        text = raw.strip() if hasattr(raw, "strip") else str(raw)
    except Exception:
        # API å‘¼ã³å‡ºã—å¤±æ•—æ™‚ã¯ã™ã¹ã¦ç©ºã‚¿ã‚°ã§åŸ‹ã‚ã‚‹
        return [("", "") for _ in comments]

    lines = [line for line in text.splitlines() if line.strip()]
    results: list[tuple[str, str]] = []
    for line in lines[: len(comments)]:
        # "1. ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°: xxx | ã‚µãƒ–ã‚¿ã‚°: yyy, zzz"
        try:
            _, rest = line.split(".", 1)
            main_part, sub_part = rest.split("|", 1)
            main = main_part.split(":", 1)[1].strip()
            subs = [t.strip() for t in sub_part.split(":", 1)[1].split(",")][:2]
            sub = ", ".join(subs)
        except Exception:
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ç©ºæ–‡å­—
            main, sub = "", ""
        results.append((main, sub))
    # è¶³ã‚Šãªã„åˆ†ã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
    while len(results) < len(comments):
        results.append(("", ""))
    return results

# â”€â”€â”€ ä¸¦åˆ—ãƒãƒƒãƒå®Ÿè¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_all(comments: list[str], key: str) -> list[tuple[str, str]]:
    total = len(comments)
    out: list[tuple[str, str]] = [("", "")] * total
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
        futures = {}
        for start in range(0, total, BATCH_SIZE):
            batch = comments[start : start + BATCH_SIZE]
            fut = exe.submit(analyze_batch, batch, key)
            futures[fut] = start
        for fut in as_completed(futures):
            start = futures[fut]
            try:
                batch_res = fut.result()
                for i, res in enumerate(batch_res):
                    out[start + i] = res
            except Exception:
                # ãƒãƒƒãƒå¤±æ•—æ™‚ã¯ç©ºã‚¿ã‚°
                for i in range(start, min(start + BATCH_SIZE, total)):
                    out[i] = ("", "")
    return out

# â”€â”€â”€ ã‚¿ã‚°ä»˜ã‘å®Ÿè¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ğŸ¤– ã‚¿ã‚°ä»˜ã‘ã‚’å®Ÿè¡Œ"):
    if not api_key:
        st.error("å…ˆã«APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        st.stop()
    comments = df["ã‚³ãƒ¡ãƒ³ãƒˆ"].astype(str).tolist()
    with st.spinner("è§£æä¸­â€¦ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„"):
        pairs = analyze_all(comments, api_key)
        df["ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°"], df["ã‚µãƒ–ã‚¿ã‚°"] = zip(*pairs)
    st.success("âœ… ã‚¿ã‚°ä»˜ã‘ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

# â”€â”€â”€ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼†CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°" in df.columns:
    # KPI
    total      = len(df)
    main_counts = df["ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°"].value_counts()
    unique_main = main_counts.size
    top_main    = int(main_counts.iloc[0]) if not main_counts.empty else 0

    k1, k2, k3 = st.columns(3)
    k1.metric("ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°", f"{total}")
    k2.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°æ•°", f"{unique_main}")
    k3.metric("ãƒˆãƒƒãƒ—ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°ä»¶æ•°", f"{top_main}")

    st.markdown("---")

    # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    st.subheader("ãƒ¡ã‚¤ãƒ³ã‚¿ã‚° ä»¶æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    fig1, ax1 = plt.subplots(figsize=(6, 4))
    main_counts.plot.bar(ax=ax1)
    ax1.set_ylabel("ä»¶æ•°")
    st.pyplot(fig1, use_container_width=True)

    # é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¿ã‚°åˆ¥ï¼‰
    st.subheader("ãƒ¡ã‚¤ãƒ³ã‚¿ã‚° é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰")
    weekly = (
        df.set_index("ä½œæˆæ—¥")["ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°"]
          .groupby(pd.Grouper(freq="W"))
          .value_counts()
          .unstack(fill_value=0)
    )
    st.line_chart(weekly, use_container_width=True)

    # æ–°è¦ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°æ¤œå‡º
    first = df["ä½œæˆæ—¥"].min()
    baseline_end = first + datetime.timedelta(days=6)
    baseline_tags = set(df[df["ä½œæˆæ—¥"] <= baseline_end]["ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°"])
    all_tags = set(df["ãƒ¡ã‚¤ãƒ³ã‚¿ã‚°"])
    new_tags = sorted(all_tags - baseline_tags)
    if new_tags:
        st.subheader("éå»æœªå‡ºãƒ¡ã‚¤ãƒ³ã‚¿ã‚°")
        for t in new_tags:
            st.write(f"- {t}")

    st.markdown("---")

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv_data = df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "ğŸ“¥ ã‚¿ã‚°ä»˜ãCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        csv_data,
        "tagged_results.csv",
        mime="text/csv"
    )
else:
    st.info("ã‚¿ã‚°ä»˜ã‘ã‚’å®Ÿè¡Œã™ã‚‹ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
